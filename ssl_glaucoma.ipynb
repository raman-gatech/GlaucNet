{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc22928-0af9-4ba8-93b5-1afb69e0d628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Loss: 0.9999773472547531\n",
      "Epoch [2/50] - Loss: 0.9999773472547531\n",
      "Epoch [3/50] - Loss: 0.9999773472547531\n",
      "Epoch [4/50] - Loss: 0.9999773472547531\n",
      "Epoch [5/50] - Loss: 0.9999773472547531\n",
      "Epoch [6/50] - Loss: 0.9999773472547531\n",
      "Epoch [7/50] - Loss: 0.9999773472547531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Forward pass for both augmented views\u001b[39;00m\n\u001b[32m    105\u001b[39m projections1 = simclr_model(augmented_images1)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m projections2 = \u001b[43msimclr_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_images2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Labels: 1 if they belong to the same class, 0 otherwise (for simplicity, use dummy labels for now)\u001b[39;00m\n\u001b[32m    109\u001b[39m labels = torch.ones(images.size(\u001b[32m0\u001b[39m)).to(device)  \u001b[38;5;66;03m# All pairs are assumed to be positive pairs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mSimCLR.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     projections = \u001b[38;5;28mself\u001b[39m.projection_head(features)\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m projections\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torchvision/models/resnet.py:273\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    270\u001b[39m x = \u001b[38;5;28mself\u001b[39m.relu(x)\n\u001b[32m    271\u001b[39m x = \u001b[38;5;28mself\u001b[39m.maxpool(x)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer2(x)\n\u001b[32m    275\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer3(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torchvision/models/resnet.py:96\u001b[39m, in \u001b[36mBasicBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     93\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn1(out)\n\u001b[32m     94\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn2(out)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Custom Dataset for SSL (SimCLR)\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Get all image files in the root directory\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith(('.jpg', '.png'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "# Define the ContrastiveLoss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, 2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Define the SimCLR model (assuming the base model is a ResNet)\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim=128):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        # Get the number of input features for the last fully connected layer before identity replacement\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()  # Remove final classification layer\n",
    "        \n",
    "        # Improved projection head with batch normalization\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),  # Use the number of features in the last layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),  # Batch normalization\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        projections = self.projection_head(features)\n",
    "        return projections\n",
    "\n",
    "\n",
    "# Define the data augmentations for SimCLR\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),  # Increased strength of augmentations\n",
    "    transforms.RandomRotation(45),  # Increased rotation angle\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load your unlabeled dataset (replace with your actual path)\n",
    "train_dataset = CustomImageDataset(root_dir='/Users/ramanathanswaminathan/Downloads/glaucoma_exhaustive/acrima+drishti_ssl/Training', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize the SimCLR model and contrastive loss\n",
    "base_model = models.resnet18(weights='IMAGENET1K_V1')  # Update to use correct pretrained weights\n",
    "simclr_model = SimCLR(base_model)\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(simclr_model.parameters(), lr=1e-4)  # Adjusted learning rate\n",
    "\n",
    "# Training loop for SimCLR pretraining\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "simclr_model = simclr_model.to(device)\n",
    "simclr_model.train()\n",
    "\n",
    "for epoch in range(50):  # Pretrain for 50 epochs\n",
    "    running_loss = 0.0\n",
    "    for images in train_loader:  # No labels are used here\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # SimCLR requires pairs of augmented images (positive pairs)\n",
    "        augmented_images1 = images\n",
    "        augmented_images2 = images  # For simplicity, we'll treat images as paired in this basic setup\n",
    "        \n",
    "        # Forward pass for both augmented views\n",
    "        projections1 = simclr_model(augmented_images1)\n",
    "        projections2 = simclr_model(augmented_images2)\n",
    "        \n",
    "        # Labels: 1 if they belong to the same class, 0 otherwise (for simplicity, use dummy labels for now)\n",
    "        labels = torch.ones(images.size(0)).to(device)  # All pairs are assumed to be positive pairs\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        loss = contrastive_loss(projections1, projections2, labels)\n",
    "        \n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/50] - Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12efeb-73c1-4f80-9f14-7ce28df5a934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e00cf7-dc1d-40bb-bc54-a67b612aff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16895fe5-63cc-4f8d-954d-3e06583e92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the model on the labeled dataset (Glaucoma detection)\n",
    "# Modify the model to include a final classification layer for 2 classes\n",
    "num_ftrs = simclr_model.base_model.fc.in_features\n",
    "simclr_model.base_model.fc = nn.Linear(num_ftrs, 2)  # 2 classes: glaucoma and normal\n",
    "\n",
    "# Set the optimizer (fine-tuning)\n",
    "optimizer = optim.Adam(simclr_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load the labeled dataset (replace with the path to your actual labeled data)\n",
    "train_dataset_finetune = datasets.ImageFolder(root='/Users/ramanathanswaminathan/Downloads/glaucoma_exhaustive/acrima+drishti/Training', transform=transform)\n",
    "train_loader_finetune = DataLoader(train_dataset_finetune, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Fine-tuning loop\n",
    "simclr_model.train()\n",
    "for epoch in range(10):  # Fine-tune for 10 epochs\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader_finetune:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = simclr_model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Fine-tuning Epoch [{epoch+1}/10] - Loss: {running_loss/len(train_loader_finetune)}\")\n",
    "\n",
    "# Evaluation of the fine-tuned model\n",
    "# Assuming you have a separate test set for evaluation\n",
    "test_dataset = datasets.ImageFolder(root='/Users/ramanathanswaminathan/Downloads/glaucoma_exhaustive/acrima+drishti/Testing', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Evaluate the model\n",
    "simclr_model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = simclr_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {accuracy}%')\n",
    "\n",
    "# Optional: Save the model for future use\n",
    "torch.save(simclr_model.state_dict(), \"simclr_glaucoma_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02887017-c396-47c3-84c8-9d7a26c31155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262be6da-beaa-43cb-a379-51da7f9a78b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eea3a5-ceb1-496a-a486-317cc7d05fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45e8f9-8948-43a1-b200-c23872858f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a0838-daae-424d-98fc-ea26da76fd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191cda3-360f-4cd4-ac7e-364728a0e99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f3d2e-4b7f-4e95-a06a-81d4f24ea0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4af14-03af-4534-9de5-fb6ba7abab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssl plus ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31e7a7-2752-4d13-a5a7-48e7da422574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from transformers import ViTModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data augmentation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2))\n",
    "])\n",
    "\n",
    "# Dataset paths\n",
    "train_dir = \"/Users/ramanathanswaminathan/Downloads/glaucoma_exhaustive/acrima+drishti/Training\"\n",
    "test_dir = \"/Users/ramanathanswaminathan/Downloads/glaucoma_exhaustive/acrima+drishti/Testing\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# SimCLR Model (Pretrained)\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim=128):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        projections = self.projection_head(features)\n",
    "        return projections\n",
    "\n",
    "# Load pre-trained SimCLR model (e.g., ResNet-18)\n",
    "base_model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "simclr_model = SimCLR(base_model).to(device)\n",
    "\n",
    "# Hybrid Model 1 (EfficientNet + ViT)\n",
    "class HybridModel1(nn.Module):\n",
    "    def __init__(self, feature_dim=768, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = models.efficientnet_b0(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])\n",
    "        \n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\", add_pooling_layer=False)\n",
    "        self.projection = nn.Linear(1280 + feature_dim, feature_dim)\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "        self.batch_norm = nn.BatchNorm1d(feature_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn(x).flatten(1)\n",
    "        vit_features = self.vit(x).last_hidden_state[:, 0, :]\n",
    "        features = torch.cat([cnn_features, vit_features], dim=-1)\n",
    "        fused_features = self.projection(features)\n",
    "        \n",
    "        attn_output, _ = self.cross_attention(fused_features.unsqueeze(1), fused_features.unsqueeze(1), fused_features.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        \n",
    "        attention_scores = torch.sigmoid(self.attention_weights(attn_output))\n",
    "        weighted_features = attn_output * attention_scores\n",
    "        \n",
    "        weighted_features = self.batch_norm(weighted_features)\n",
    "        weighted_features = self.dropout(weighted_features)\n",
    "        return self.fc(weighted_features)\n",
    "\n",
    "# Hybrid Model 2 (ResNet + ViT)\n",
    "class HybridModel2(nn.Module):\n",
    "    def __init__(self, feature_dim=768, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet150(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])\n",
    "        \n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\", add_pooling_layer=False)\n",
    "        self.projection = nn.Linear(512 + feature_dim, feature_dim)\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=feature_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "        self.batch_norm = nn.BatchNorm1d(feature_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn(x).flatten(1)\n",
    "        vit_features = self.vit(x).last_hidden_state[:, 0, :]\n",
    "        features = torch.cat([cnn_features, vit_features], dim=-1)\n",
    "        fused_features = self.projection(features)\n",
    "        \n",
    "        attn_output, _ = self.cross_attention(fused_features.unsqueeze(1), fused_features.unsqueeze(1), fused_features.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        \n",
    "        attention_scores = torch.sigmoid(self.attention_weights(attn_output))\n",
    "        weighted_features = attn_output * attention_scores\n",
    "        \n",
    "        weighted_features = self.batch_norm(weighted_features)\n",
    "        weighted_features = self.dropout(weighted_features)\n",
    "        return self.fc(weighted_features)\n",
    "\n",
    "# Ensemble Model: Combine Predictions of Two Models and SimCLR\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2, ssl_model):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.ssl_model = ssl_model  # SimCLR Model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get predictions from both models\n",
    "        pred1 = self.model1(x)\n",
    "        pred2 = self.model2(x)\n",
    "        \n",
    "        # Get features from the SimCLR model (projection output)\n",
    "        ssl_features = self.ssl_model(x)\n",
    "        \n",
    "        # Combine predictions: Averaging the outputs and concatenating SSL features\n",
    "        combined_pred = (pred1 + pred2) / 2\n",
    "        combined_pred = torch.cat([combined_pred, ssl_features], dim=-1)  # Concatenate SSL features for final decision\n",
    "        return combined_pred\n",
    "\n",
    "# Training function with Cyclical Learning Rate, Gradual Layer Freezing, and Label Smoothing\n",
    "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, device, epochs=150):\n",
    "    train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "    best_test_accuracy = 0.0\n",
    "    best_model_state_dict = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train, total_train = 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            correct_train += (outputs.argmax(1) == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        train_accuracy = correct_train / total_train * 100\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Testing Phase\n",
    "        model.eval()\n",
    "        correct_test, total_test = 0, 0\n",
    "        running_test_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "                correct_test += (outputs.argmax(1) == labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "                all_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        test_accuracy = correct_test / total_test * 100\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f} - Train Acc: {train_accuracy:.2f}% - Test Loss: {avg_test_loss:.4f} - Test Acc: {test_accuracy:.2f}%\")\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save the model with the best test accuracy\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            print(f\"New best model with Test Acc: {best_test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save the best model after training completes\n",
    "    if best_model_state_dict is not None:\n",
    "        torch.save(best_model_state_dict, \"best_ensemble_model_with_ssl.pth\")\n",
    "        print(\"Best model saved with Test Accuracy: {:.2f}%\".format(best_test_accuracy))\n",
    "    \n",
    "    # Plotting training and testing curves\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), test_losses, label='Test Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(epochs), test_accuracies, label='Test Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Confusion Matrix and other metrics\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    # ROC and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Initialize models\n",
    "model1 = HybridModel1().to(device)\n",
    "model2 = HybridModel2().to(device)\n",
    "\n",
    "# Optimizer, loss, and scheduler\n",
    "optimizer = optim.AdamW(list(model1.parameters()) + list(model2.parameters()) + list(simclr_model.parameters()), lr=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Create and train ensemble model\n",
    "ensemble_model = EnsembleModel(model1, model2, simclr_model).to(device)\n",
    "train_model(ensemble_model, train_loader, test_loader, optimizer, scheduler, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7d4af-a5bf-4ba7-a1dd-85b3be755000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
